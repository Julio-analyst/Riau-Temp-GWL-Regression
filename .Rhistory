par(mfrow = c(1, 3))
plot(gam.lr, se = TRUE, col = "green")
pred_probs <- predict(gam.lr, newdata = test_data, type = "response")
pred_class <- ifelse(pred_probs > 0.5, 1, 0)
actual_class <- ifelse(test_data$wage > 250, 1, 0)
# Akurasi model
accuracy <- mean(pred_class == actual_class)
print(accuracy)
library(ISLR)
library(gam)
attach(Collage)
attach(collage)
library(ISLR)
library(gam)
library(ISLR)
library(gam)
```{r}
attach(collage)
attach(Collage)
data(Collage)
data(collage)
data("Collage")
data("College")
attach(College)
View(College)
View(College)
attach(College)
gam1=lm(wage~ns(year,4)+ns(age,5)+education, data=College)
gam.m3=gam(wage~s(year,4)+s(age, 4)+education, data=College)
par(mfrow=c(1,3))
plot(gam.m3, se=T, col='blue')
par(mfrow=c(1,3))
plot.Gam(gam1, se=T, col='red')
gam.m1=gam(wage~s(age, 5)+education, data=Wage)
gam.m2=gam(wage~year+s(age, 5)+education, data=Wage)
anova(gam.m1, gam.m2, gam.m3)
summary(gam.m3)
pred=predict(gam.m2, newdata = Wage)
gam.lo=gam(wage~s(year, df=4)+lo(age, span=0.7)+education, data=Wage)
par(mfrow=c(1,3))
plot(gam.lo, se=T, col='green')
install.packages("akima")
install.packages("interp")
install.packages("interp")
library(akima)
library(interp)
gam.lo.i=gam(wage~lo(year, age, span=0.5)+education, data=Wage)
plot(gam.lo.i)
gam.lr=gam(I(wage>250)~year+s(age, df=5)+education, family=binomial, data=Wage)
par(mfrow=c(1, 3))
plot(gam.lr, se=T, col='red')
table(education, I(wage>250))
gam.lr.s=gam(I(wage>250)~year+s(age, df=5)+education, family=binomial, data=Wage, suset=( education!='1.<HS Grad'))
par(mfrow=c(1,3))
plot(gam.lr.s, se=T, col='green')
set.seed(123)
train_idx <- sample(1:nrow(Wage), 0.7 * nrow(Wage))
train_data <- Wage[train_idx, ]
test_data <- Wage[-train_idx, ]
# Forward Selection secara bertahap (contoh manual)
m1 <- gam(I(wage > 250) ~ education, family = binomial, data = train_data)
m2 <- gam(I(wage > 250) ~ education + year, family = binomial, data = train_data)
m3 <- gam(I(wage > 250) ~ education + year + s(age, df=5), family = binomial, data = train_data)
anova(m1, m2, m3, test = "F")
gam.lr = gam(I(wage > 250) ~ year + s(age, df = 5) + education,
family = binomial, data = train_data)
par(mfrow = c(1, 3))
plot(gam.lr, se = TRUE, col = "green")
pred_probs <- predict(gam.lr, newdata = test_data, type = "response")
pred_class <- ifelse(pred_probs > 0.5, 1, 0)
actual_class <- ifelse(test_data$wage > 250, 1, 0)
# Akurasi model
accuracy <- mean(pred_class == actual_class)
print(accuracy)
gam.lo.i=gam(College~lo(year, age, span=0.5)+education, data=College)
gam1=lm(College~ns(year,4)+ns(age,5)+education, data=College)
library(ISLR)
library(gam)
set.seed(123)
# Pisahkan data
data <- College
train_index <- sample(1:nrow(data), 0.7 * nrow(data))
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
# Model dasar
m0 <- gam(Outstate ~ 1, data = train_data)
m1 <- gam(Outstate ~ Private, data = train_data)
m2 <- gam(Outstate ~ Private + Room.Board, data = train_data)
m3 <- gam(Outstate ~ Private + Room.Board + s(PhD), data = train_data)
anova(m0, m1, m2, m3, test = "F")
model_gam <- gam(Outstate ~ Private + Room.Board + s(PhD), data = train_data)
par(mfrow = c(1, 2))
plot(model_gam, se = TRUE, col = "blue")
prediksi <- predict(model_gam, newdata = test_data)
mse <- mean((test_data$Outstate - prediksi)^2)
print(mse)
prediksi <- predict(model_gam, newdata = test_data)
mse <- mean((test_data$Outstate - prediksi)^2)
print(mse)
library (ISLR)
library (gam)
library (mgcv)
library (leaps)
attach(College)
View(College)
# Memisahkan data menjadi training dan test
set.seed(123)
train_index <- sample(1:nrow(College), 0.7 *nrow(College))
train_data <- College[train_index, ]
test_data <- College[-train_index, ]
# Forward Selection
regfit_fwd <- regsubsets(Outstate~.,data=train_data,nvmax=19,method="forward")
reg_summary <- summary(regfit_fwd)
# Plot adjusted r-squared untuk memilih model terbaik
plot(reg_summary$adjr2, type = "b", xlab = "Numbers of predictors", ylab = "Adjusted R^2")
best_model_size <-which.max (reg_summary$adjr2)
best_model_size
best_model <- summary(regfit_fwd)$which[best_model_size, ]
selected_predictors <- names(best_model[best_model][-1])[best_model[-1]]
selected_predictors
regfit.fwd <- regsubsets(Outstate ~.,data = train_data, nvmax = 17, method = "forward")
# Hasil selesksi model
summary_fwd <- summary (regfit.fwd)
# Menentukan Model terbaik berdasarkan kriteria
which.max(summary_fwd$adjr2)
best_model_size <-which.max(summary_fwd$adjr2)
#prediktor terpilih
coef(regfit.fwd, best_model_size)
gam.model <- gam(Outstate~ Private + s(Room.Board) + s(PhD) + s(perc.alumni), data = train_data)
#visualisai plot gam
par(mfrow = c(2,2))
plot(gam.model, se = TRUE, col = "blue")
#prediksi data test
pred <- predict (gam.model, newdata = test)
library (ISLR)
library (gam)
library (mgcv)
library (leaps)
library (dplyr)
library (caret)
library (ISLR)
library (gam)
library (mgcv)
library (leaps)
library (dplyr)
library (caret)
library(tidyverse)
library(caret)
library(MASS)
# Misalnya data bernama 'collage_data'
set.seed(123)
# Pisahkan data menjadi training dan testing
train_index <- createDataPartition(collage_data$out_of_state_tuition, p = 0.7, list = FALSE)
library (ISLR)
library (gam)
library (mgcv)
library (leaps)
library (dplyr)
library (caret)
library(tidyverse)
library(caret)
library(MASS)
# Misalnya data bernama 'collage_data'
set.seed(123)
college_data = College
# Pisahkan data menjadi training dan testing
train_index <- createDataPartition(collage_data$out_of_state_tuition, p = 0.7, list = FALSE)
library (ISLR)
library (gam)
library (mgcv)
library (leaps)
library (dplyr)
library (caret)
library(tidyverse)
library(caret)
library(MASS)
# Misalnya data bernama 'collage_data'
set.seed(123)
college_data = College
# Pisahkan data menjadi training dan testing
train_index <- createDataPartition(collage_data$out_of_state_tuition, p = 0.7, list = FALSE)
library (ISLR)
library (gam)
library (mgcv)
library (leaps)
library (dplyr)
library (caret)
library(tidyverse)
library(caret)
library(MASS)
# Misalnya data bernama 'collage_data'
set.seed(123)
college_data = College
# Pisahkan data menjadi training dan testing
train_index <- createDataPartition(collage_data$out_of_state_tuition, p = 0.7, list = FALSE)
library (ISLR)
library (gam)
library (mgcv)
library (leaps)
attach(College)
View(College)
# Memisahkan data menjadi training dan test
set.seed(123)
train_index <- sample(1:nrow(College), 0.7 *nrow(College))
train_data <- College[train_index, ]
test_data <- College[-train_index, ]
# Forward Selection
regfit_fwd <- regsubsets(Outstate~.,data=train_data,nvmax=19,method="forward")
reg_summary <- summary(regfit_fwd)
# Plot adjusted r-squared untuk memilih model terbaik
plot(reg_summary$adjr2, type = "b", xlab = "Numbers of predictors", ylab = "Adjusted R^2")
best_model_size <-which.max (reg_summary$adjr2)
best_model_size
best_model <- summary(regfit_fwd)$which[best_model_size, ]
selected_predictors <- names(best_model[best_model][-1])[best_model[-1]]
selected_predictors
regfit.fwd <- regsubsets(Outstate ~.,data = train_data, nvmax = 17, method = "forward")
# Hasil selesksi model
summary_fwd <- summary (regfit.fwd)
# Menentukan Model terbaik berdasarkan kriteria
which.max(summary_fwd$adjr2)
best_model_size <-which.max(summary_fwd$adjr2)
#prediktor terpilih
coef(regfit.fwd, best_model_size)
gam.model <- gam(Outstate~ Private + s(Room.Board) + s(PhD) + s(perc.alumni), data = train_data)
#visualisai plot gam
par(mfrow = c(2,2))
plot(gam.model, se = TRUE, col = "blue")
#prediksi data test
pred <- predict (gam.model, newdata = test)
#prediksi data test
pred <- predict (gam.model, newdata = test)
#prediksi data test
pred <- predict (gam.model, newdata = test_data)
#hitung mse
mse <- mean((test$Outstate.pred)^2)
#prediksi data test
pred <- predict (gam.model, newdata = test_data)
#hitung mse
mse <- mean((test_data$Outstate.pred)^2)
print(paste)
# Prediksi data test
pred <- predict(gam.model, newdata = test_data)
# Hitung MSE dengan membandingkan prediksi dan nilai aktual
mse <- mean((test_data$Outstate - pred)^2)
# Tampilkan hasil MSE
print(paste("Mean Squared Error (MSE):", round(mse, 2)))# Prediksi data test
pred <- predict(gam.model, newdata = test_data)
# Hitung MSE dengan membandingkan prediksi dan nilai aktual
mse <- mean((test_data$Outstate - pred)^2)
# Tampilkan hasil MSE
print(paste("Mean Squared Error (MSE):", round(mse, 2)))
library(class)
library(caret)
library(dplyr)
library(ggplot2)
library(GGally)
library(tidyr)
library(ROSE)
df <- read.csv("C:/Users/Farrel Julio/OneDrive/ドキュメント/SSD/SSD 2025/DATASET/rumahbalam_kategorik_strategis.csv")
sum(is.na(df))
sum(duplicated(df))
features <- c('Jarakm', 'LuasBangunanm2', 'LuasTanahm2',
'KamarTidur', 'KamarMandi', 'Garasi',
'DayaListrik.Watt')
target <- "LokasiStrategis"
# Histogram
for (feature in features) {
print(
ggplot(df, aes_string(x = feature)) +
geom_histogram(fill = "steelblue", bins = 30, color = "black") +
labs(title = paste("Histogram dari", feature)) +
theme_minimal()
)
}
# Korelasi
ggpairs(df[, features], title = "Scatter Plot Matrix antar Fitur Numerik")
# Boxplot
for (feature in features) {
print(
ggplot(df, aes_string(x = target, y = feature, fill = target)) +
geom_boxplot() +
labs(title = paste("Boxplot", feature, "berdasarkan LokasiStrategis")) +
theme_minimal()
)
}
df_scaled <- df
df_scaled[features] <- scale(df[features])
set.seed(42)
splitIndex <- createDataPartition(df_scaled[[target]], p = 0.8, list = FALSE)
train_data <- df_scaled[splitIndex, ]
test_data <- df_scaled[-splitIndex, ]
train_X <- train_data[, features]
train_Y <- train_data[[target]]
test_X <- test_data[, features]
test_Y <- test_data[[target]]
eval_metrics <- function(actual, predicted) {
cm <- table(Actual = actual, Predicted = predicted)
tn <- cm["Tidak", "Tidak"]
tp <- cm["Ya", "Ya"]
fn <- cm["Ya", "Tidak"]
fp <- cm["Tidak", "Ya"]
acc <- (tp + tn) / sum(cm)
sens <- tp / (tp + fn)
spec <- tn / (tn + fp)
return(c(accuracy = acc, sensitivity = sens, specificity = spec))
}
results <- data.frame()
for (k in 1:10) {
pred_train <- knn(train = train_X, test = train_X, cl = train_Y, k = k)
pred_test <- knn(train = train_X, test = test_X, cl = train_Y, k = k)
train_metrics <- eval_metrics(train_Y, pred_train)
test_metrics <- eval_metrics(test_Y, pred_test)
results <- rbind(results, data.frame(
k = k,
TrainAccuracy = train_metrics["accuracy"],
TestAccuracy = test_metrics["accuracy"],
TrainSensitivity = train_metrics["sensitivity"],
TestSensitivity = test_metrics["sensitivity"],
TrainSpecificity = train_metrics["specificity"],
TestSpecificity = test_metrics["specificity"]
))
}
print(results)
ggplot(results, aes(x = k)) +
geom_line(aes(y = TrainAccuracy, color = "Train Accuracy"), size = 1.2) +
geom_line(aes(y = TestAccuracy, color = "Test Accuracy"), size = 1.2) +
geom_point(aes(y = TrainAccuracy, color = "Train Accuracy"), size = 2) +
geom_point(aes(y = TestAccuracy, color = "Test Accuracy"), size = 2) +
scale_color_manual(values = c("Train Accuracy" = "blue", "Test Accuracy" = "red")) +
labs(title = "Akurasi KNN pada Data Latih vs Uji", x = "Jumlah Tetangga (k)", y = "Akurasi") +
theme_minimal()
resample_results <- data.frame()
for (k in c(5)) {
pred_holdout <- knn(train = train_X, test = test_X, cl = train_Y, k = k)
met_holdout <- eval_metrics(test_Y, pred_holdout)
resample_results <- rbind(resample_results, data.frame(k = k, Resampling = "Holdout", t(met_holdout)))
folds <- createFolds(df_scaled[[target]], k = 5)
accs <- sens <- specs <- numeric(5)
for (i in 1:5) {
test_idx <- folds[[i]]
train_idx <- setdiff(seq_len(nrow(df_scaled)), test_idx)
train_X_cv <- df_scaled[train_idx, features]
train_Y_cv <- df_scaled[train_idx, target]
test_X_cv <- df_scaled[test_idx, features]
test_Y_cv <- df_scaled[test_idx, target]
pred_cv <- knn(train = train_X_cv, test = test_X_cv, cl = train_Y_cv, k = k)
m <- eval_metrics(test_Y_cv, pred_cv)
accs[i] <- m["accuracy"]
sens[i] <- m["sensitivity"]
specs[i] <- m["specificity"]
}
resample_results <- rbind(resample_results, data.frame(k = k, Resampling = "Cross Validation",
Accuracy = mean(accs), Sensitivity = mean(sens), Specificity = mean(specs)))
}
resample_results <- data.frame()
for (k in c(5)) {
# HOLDOUT
pred_holdout <- knn(train = train_X, test = test_X, cl = train_Y, k = k)
met_holdout <- eval_metrics(test_Y, pred_holdout)
resample_results <- rbind(resample_results, data.frame(
k = k,
Resampling = "Holdout",
Accuracy = met_holdout["accuracy"],
Sensitivity = met_holdout["sensitivity"],
Specificity = met_holdout["specificity"]
))
# CROSS VALIDATION
folds <- createFolds(df_scaled[[target]], k = 5)
accs <- sens <- specs <- numeric(5)
for (i in 1:5) {
test_idx <- folds[[i]]
train_idx <- setdiff(seq_len(nrow(df_scaled)), test_idx)
train_X_cv <- df_scaled[train_idx, features]
train_Y_cv <- df_scaled[train_idx, target]
test_X_cv <- df_scaled[test_idx, features]
test_Y_cv <- df_scaled[test_idx, target]
pred_cv <- knn(train = train_X_cv, test = test_X_cv, cl = train_Y_cv, k = k)
m <- eval_metrics(test_Y_cv, pred_cv)
accs[i] <- m["accuracy"]
sens[i] <- m["sensitivity"]
specs[i] <- m["specificity"]
}
resample_results <- rbind(resample_results, data.frame(
k = k,
Resampling = "Cross Validation",
Accuracy = mean(accs),
Sensitivity = mean(sens),
Specificity = mean(specs)
))
}
# 1. Import Data dan library
library(ggplot2)
library(readr)
data <- read.csv("C:/project/ads/Tugas Besar ADS_Kelompok 4_RB/data.xlsx, header = TRUE)
# 2. Menampilkan Summary Deskriptif
summary(data)
# 1. Import Data dan library
library(ggplot2)
library(readr)
data <- read.csv("C:/project/ads/Tugas Besar ADS_Kelompok 4_RB/data.xlsx, header = TRUE)
# 1. Import Data dan library
library(ggplot2)
library(readr)
data <- read.csv("C:/project/ads/Tugas Besar ADS_Kelompok 4_RB/data.csv, header = TRUE)
# 1. Import libraries & Baca file Excel
library(ggplot2)
library(readr)
library(readxl)  # khusus untuk file .xlsx
data <- read_excel("C:/project/ads/Tugas Besar ADS_Kelompok 4_RB/data.xlsx")
# 2. Menampilkan Summary Deskriptif
summary(data)
# 3. Scatter Plot
plot(data$Suhu, data$GWL,
main = "Scatter Plot Suhu vs GWL",
xlab = "Suhu (C)", ylab = "GWL (m)",
pch = 19, col = "blue")
# Import libraries
library(ggplot2)
library(readr)  # Lebih cepat dari read.csv
# Baca file CSV
data <- read_csv("C:/project/ads/Tugas Besar ADS_Kelompok 4_RB/data.csv")
# 2. Menampilkan Summary Deskriptif
summary(data)
# 3. Scatter Plot
plot(data$Suhu, data$GWL,
main = "Scatter Plot Suhu vs GWL",
xlab = "Suhu (C)", ylab = "GWL (m)",
pch = 19, col = "blue")
# 2. Menampilkan Summary Deskriptif
str(data)
summary(data)
# 3. Scatter Plot
plot(data$Suhu, data$GWL,
main = "Scatter Plot Suhu vs GWL",
xlab = "Suhu (C)", ylab = "GWL (m)",
pch = 19, col = "blue")
# 3. Scatter Plot
plot(data$Suhu (celcius), data$GWL (m)      ,
main = "Scatter Plot Suhu vs GWL",
xlab = "Suhu (C)", ylab = "GWL (m)",
pch = 19, col = "blue")
# 3. Scatter Plot
plot(data$'Suhu (celcius)', 'data$GWL (m)'      ,
main = "Scatter Plot Suhu vs GWL",
xlab = "Suhu (C)", ylab = "GWL (m)",
pch = 19, col = "blue")
# 3. Scatter Plot
plot(
data$`Suhu (celcius)`, data$`GWL (m)`,
main = "Scatter Plot Suhu vs GWL",
xlab = "Suhu (C)", ylab = "GWL (m)",
pch = 19, col = "blue"
)
# 3. Scatter Plot
plot(
data$`Suhu (celcius)`, data$`GWL (m)`,
main = "Scatter Plot Suhu vs GWL",
xlab = "Suhu (C)", ylab = "GWL (m)",
pch = 1, col = "blue"
)
# 4. Membuat Model Regresi
model <- lm(GWL ~ Suhu, data = data)
# 4. Membuat Model Regresi
model <- lm(`GWL (m)` ~ `Suhu (celcius)`, data = data)
# 5. Garis Regresi
abline(model, col = "red", lwd = 2)
# 6. Summary Model Regresi
summary(model)
# 4. Membuat Model Regresi
model <- lm(`GWL (m)` ~ `Suhu (celcius)`, data = data)
model
# 4. Membuat Model Regresi
model_regresi <- lm(`GWL (m)` ~ `Suhu (celcius)`, data = data)
model_regresi
# Scatter plot dulu
plot(data$`Suhu (celcius)`, data$`GWL (m)`,
main = "Scatter Plot Suhu vs GWL",
xlab = "Suhu (C)", ylab = "GWL (m)",
pch = 1, col = "blue")  # pch = 1 untuk lingkaran bolong
# Baru tambahkan garis regresi
abline(model, col = "red", lwd = 2)
# 4. Membuat Model Regresi
model_regresi <-lm(formula = data$"Suhu (celcius)" ~ data$"GWL (m)")
model_regresi
# 5. Membuat Garis Regresi
plot(data$`Suhu (celcius)` ~ data$`GWL (m)`,
col = "blue",
pch = 1,  # lingkaran bolong
main = "Scatter Plot: Suhu vs GWL",
xlab = "GWL (m)",
ylab = "Suhu (°C)")
# Menambahkan garis regresi
abline(lm(data$`Suhu (celcius)` ~ data$`GWL (m)`), col = "red", lwd = 2)
# 6. Summary Model Regresi
summary(model_regresi)
# 7. Korelasi Pearson
# Korelasi antara Suhu dan GWL
cor(x = data$`Suhu (celcius)`, y = data$`GWL (m)`)
# 8. Koefisien Determinasi Manual
# Membuat model regresi terlebih dahulu
model_regresi <- lm(data$`Suhu (celcius)` ~ data$`GWL (m)`)
# Menampilkan koefisien determinasi dari model
summary(model_regresi)$r.squared
bptest(model_regresi)
# 9.Install (jika belum)
install.packages("lmtest")
# Load package
library(lmtest)
# Baru jalankan uji Breusch-Pagan
bptest(model_regresi)
chooseCRANmirror()
